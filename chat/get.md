# Get chat completion

Source: https://platform.openai.com/docs/api-reference/chat/get

`GET /v1/chat/completions/{completion_id}`

Get a stored chat completion. Only Chat Completions that have been created
with the `store` parameter set to `true` will be returned.

## Parameters
- `completion_id` (path, string, required): The ID of the chat completion to retrieve.

## Responses
### 200
A chat completion
#### application/json
- `id` (string, required): A unique identifier for the chat completion.
- `choices` (array<object>, required): A list of chat completion choices. Can be more than one if `n` is greater than 1.
  - Items:
    - `finish_reason` (string, required): The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
      `length` if the maximum number of tokens specified in the request was reached,
      `content_filter` if content was omitted due to a flag from our content filters,
      `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function. Enum: 'stop', 'length', 'tool_calls', 'content_filter', 'function_call'.
    - `index` (integer, required): The index of the choice in the list of choices.
    - `message` (object, required): A chat completion message generated by the model.
      - `content` (string | null, required)
      - `refusal` (string | null, required)
      - `tool_calls` (array<object>, optional): The tool calls generated by the model, such as function calls.
      - `annotations` (array<object>, optional): Annotations for the message, when applicable, as when using the
        [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
        - Items:
          - ...
      - `role` (string, required): The role of the author of this message. Enum: 'assistant'.
      - `function_call` (object, optional): Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model. Deprecated.
        - `arguments` (string, required): The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
        - `name` (string, required): The name of the function to call.
      - `audio` (object | null, optional)
        - Variant (object):
          - ...
    - `logprobs` (object | null, required)
      - Variant (object):
        - `content` (array<object> | null, required)
        - `refusal` (array<object> | null, required)
- `created` (integer, required): The Unix timestamp (in seconds) of when the chat completion was created.
- `model` (string, required): The model used for the chat completion.
- `service_tier` (string | null, optional)
- `system_fingerprint` (string, optional): This fingerprint represents the backend configuration that the model runs with.
  
  Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. Deprecated.
- `object` (string, required): The object type, which is always `chat.completion`. Enum: 'chat.completion'.
- `usage` (object, optional): Usage statistics for the completion request.
  - `completion_tokens` (integer, required): Number of tokens in the generated completion. Default: `0`.
  - `prompt_tokens` (integer, required): Number of tokens in the prompt. Default: `0`.
  - `total_tokens` (integer, required): Total number of tokens used in the request (prompt + completion). Default: `0`.
  - `completion_tokens_details` (object, optional): Breakdown of tokens used in a completion.
    - `accepted_prediction_tokens` (integer, optional): When using Predicted Outputs, the number of tokens in the
      prediction that appeared in the completion. Default: `0`.
    - `audio_tokens` (integer, optional): Audio input tokens generated by the model. Default: `0`.
    - `reasoning_tokens` (integer, optional): Tokens generated by the model for reasoning. Default: `0`.
    - `rejected_prediction_tokens` (integer, optional): When using Predicted Outputs, the number of tokens in the
      prediction that did not appear in the completion. However, like
      reasoning tokens, these tokens are still counted in the total
      completion tokens for purposes of billing, output, and context window
      limits. Default: `0`.
  - `prompt_tokens_details` (object, optional): Breakdown of tokens used in the prompt.
    - `audio_tokens` (integer, optional): Audio input tokens present in the prompt. Default: `0`.
    - `cached_tokens` (integer, optional): Cached tokens present in the prompt. Default: `0`.

## Returns
The [ChatCompletion](object.md) object matching the specified ID.

## Examples
### Example
#### Request (curl)
```bash
curl https://api.openai.com/v1/chat/completions/chatcmpl-abc123 \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json"
```
#### Request (python)
```python
from openai import OpenAI
client = OpenAI()

completions = client.chat.completions.list()
first_id = completions[0].id
first_completion = client.chat.completions.retrieve(completion_id=first_id)
print(first_completion)
```
#### Response
```json
{
  "object": "chat.completion",
  "id": "chatcmpl-abc123",
  "model": "gpt-4o-2024-08-06",
  "created": 1738960610,
  "request_id": "req_ded8ab984ec4bf840f37566c1011c417",
  "tool_choice": null,
  "usage": {
    "total_tokens": 31,
    "completion_tokens": 18,
    "prompt_tokens": 13
  },
  "seed": 4944116822809979520,
  "top_p": 1.0,
  "temperature": 1.0,
  "presence_penalty": 0.0,
  "frequency_penalty": 0.0,
  "system_fingerprint": "fp_50cad350e4",
  "input_user": null,
  "service_tier": "default",
  "tools": null,
  "metadata": {},
  "choices": [
    {
      "index": 0,
      "message": {
        "content": "Mind of circuits hum,  \nLearning patterns in silenceâ€”  \nFuture's quiet spark.",
        "role": "assistant",
        "tool_calls": null,
        "function_call": null
      },
      "finish_reason": "stop",
      "logprobs": null
    }
  ],
  "response_format": null
}
```
