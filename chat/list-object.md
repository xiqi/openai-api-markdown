# The chat completion list object

Source: https://platform.openai.com/docs/api-reference/chat/list-object

An object representing a list of Chat Completions.

## Properties
- `object` (string, required): The type of this object. It is always set to "list". Enum: 'list'. Default: `list`.
- `data` (array<object>, required): An array of chat completion objects.
  - Items:
    - `id` (string, required): A unique identifier for the chat completion.
    - `choices` (array<object>, required): A list of chat completion choices. Can be more than one if `n` is greater than 1.
      - Items:
        - `finish_reason` (string, required): The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
          `length` if the maximum number of tokens specified in the request was reached,
          `content_filter` if content was omitted due to a flag from our content filters,
          `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function. Enum: 'stop', 'length', 'tool_calls', 'content_filter', 'function_call'.
        - `index` (integer, required): The index of the choice in the list of choices.
        - `message` (object, required): A chat completion message generated by the model.
          - ...
        - `logprobs` (object | null, required)
          - Variant (object):
            - ...
    - `created` (integer, required): The Unix timestamp (in seconds) of when the chat completion was created.
    - `model` (string, required): The model used for the chat completion.
    - `service_tier` (string | null, optional)
    - `system_fingerprint` (string, optional): This fingerprint represents the backend configuration that the model runs with.
      
      Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. Deprecated.
    - `object` (string, required): The object type, which is always `chat.completion`. Enum: 'chat.completion'.
    - `usage` (object, optional): Usage statistics for the completion request.
      - `completion_tokens` (integer, required): Number of tokens in the generated completion. Default: `0`.
      - `prompt_tokens` (integer, required): Number of tokens in the prompt. Default: `0`.
      - `total_tokens` (integer, required): Total number of tokens used in the request (prompt + completion). Default: `0`.
      - `completion_tokens_details` (object, optional): Breakdown of tokens used in a completion.
        - `accepted_prediction_tokens` (integer, optional): When using Predicted Outputs, the number of tokens in the
          prediction that appeared in the completion. Default: `0`.
        - `audio_tokens` (integer, optional): Audio input tokens generated by the model. Default: `0`.
        - `reasoning_tokens` (integer, optional): Tokens generated by the model for reasoning. Default: `0`.
        - `rejected_prediction_tokens` (integer, optional): When using Predicted Outputs, the number of tokens in the
          prediction that did not appear in the completion. However, like
          reasoning tokens, these tokens are still counted in the total
          completion tokens for purposes of billing, output, and context window
          limits. Default: `0`.
      - `prompt_tokens_details` (object, optional): Breakdown of tokens used in the prompt.
        - `audio_tokens` (integer, optional): Audio input tokens present in the prompt. Default: `0`.
        - `cached_tokens` (integer, optional): Cached tokens present in the prompt. Default: `0`.
- `first_id` (string, required): The identifier of the first chat completion in the data array.
- `last_id` (string, required): The identifier of the last chat completion in the data array.
- `has_more` (boolean, required): Indicates whether there are more Chat Completions available.
